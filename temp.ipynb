{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, DataLoader2\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TabularModel(nn.Module):\n",
    "    def __init__(self, start_neurons):\n",
    "        super(TabularModel, self).__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        input_dims = [5, 6, 2, 12, 31]\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(dim, start_neurons) for dim in input_dims])\n",
    "        self.last_dense = nn.Linear(1, start_neurons)\n",
    "        \n",
    "        # Main layers\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(0.2) for _ in range(5)])\n",
    "        self.gates = nn.ModuleList([nn.Linear(start_neurons, start_neurons) for _ in range(5)])\n",
    "        self.main_denses = nn.ModuleList([nn.Linear(2 * start_neurons, 20 * start_neurons) for _ in range(5)])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(20 * start_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.embeddings):\n",
    "            embeddings.append(e(x[:, i]))\n",
    "        \n",
    "        # Last feature as dense\n",
    "        embeddings.append(self.last_dense(x[:, -1].float().unsqueeze(-1)))\n",
    "        \n",
    "        all_layer = torch.cat(embeddings, 1)\n",
    "        \n",
    "        for i in range(5):\n",
    "            all_layer_d = self.dropouts[i](all_layer)\n",
    "            all_layer_d_gate = torch.sigmoid(self.gates[i](all_layer_d))\n",
    "            all_layer_ = all_layer * all_layer_d_gate\n",
    "            all_layer_c = torch.cat([all_layer, all_layer_], 1)\n",
    "            all_layer += F.relu(self.main_denses[i](all_layer_c))\n",
    "            \n",
    "        output = self.output(all_layer).squeeze(-1)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'./data/train.csv')\n",
    "test = pd.read_csv(r'./data/test.csv')\n",
    "international_trade = pd.read_csv(r'./data/international_trade.csv')\n",
    "\n",
    "train['month'] = train['timestamp'].apply(lambda x : int(x[5:7]))\n",
    "train['day'] = train['timestamp'].apply(lambda x : int(x[8:10]))\n",
    "\n",
    "test['month'] = test['timestamp'].apply(lambda x : int(x[5:7]))\n",
    "test['day'] = test['timestamp'].apply(lambda x : int(x[8:10]))\n",
    "\n",
    "x = train.drop(columns=['ID', 'timestamp', 'supply(kg)', 'price(원/kg)'])\n",
    "y = train['price(원/kg)']\n",
    "\n",
    "x_test = test.drop(columns=['ID', 'timestamp'])\n",
    "\n",
    "qual_col = ['item', 'corporation', 'location']\n",
    "\n",
    "for i in qual_col:\n",
    "    le = LabelEncoder()\n",
    "    x[i]=le.fit_transform(x[i])\n",
    "    x_test[i]=le.transform(x_test[i])\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1103)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.values\n",
    "x_val = x_val.values\n",
    "y_train = y_train.values\n",
    "y_val = y_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x, y) -> None:\n",
    "        self.x = torch.from_numpy(x)\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        return x, y\n",
    "\n",
    "train_dataset = TabularDataset(x_train, y_train)\n",
    "val_dataset = TabularDataset(x_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train_step(self, train_loader):\n",
    "        self.model.train()\n",
    "        self.model\n",
    "        train_loss = 0\n",
    "        for data, label in train_loader:\n",
    "            data = data\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, label)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        return avg_train_loss\n",
    "\n",
    "    def validation_step(self, validation_loader):\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in iter(validation_loader):\n",
    "                data = data\n",
    "                prediction = self.model(data)\n",
    "                loss = self.criterion(prediction, data)\n",
    "                val_loss += loss.item()\n",
    "            avg_validation_loss = val_loss / len(validation_loader)\n",
    "\n",
    "        return avg_validation_loss\n",
    "\n",
    "    def fit(self, train_loader, val_loader):\n",
    "        for epoch in range(30):\n",
    "            train_loss = self.train_step(train_loader)\n",
    "            val_loss = self.validation_step(val_loader)\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{30}]\"\n",
    "                  f\"Training Loss: {train_loss:.7f} \"\n",
    "                  f\"Validation Loss: {val_loss:.7f} \"\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TabularModel(x.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "trainer = Trainer(model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Project\\jeju-price-prediction\\temp.ipynb 셀 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(train_loader, val_loader)\n",
      "\u001b[1;32mc:\\Project\\jeju-price-prediction\\temp.ipynb 셀 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, train_loader, val_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m30\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step(train_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         val_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_step(val_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m30\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.7f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidation Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m:\u001b[39;00m\u001b[39m.7f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m               )\n",
      "\u001b[1;32mc:\\Project\\jeju-price-prediction\\temp.ipynb 셀 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m data \u001b[39m=\u001b[39m data\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(output, label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\venv\\basic\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Project\\jeju-price-prediction\\temp.ipynb 셀 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m embeddings \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, e \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     embeddings\u001b[39m.\u001b[39mappend(e(x[:, i]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Last feature as dense\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/jeju-price-prediction/temp.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m embeddings\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_dense(x[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "File \u001b[1;32mc:\\venv\\basic\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\venv\\basic\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\venv\\basic\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "trainer.fit(train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
